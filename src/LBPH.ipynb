{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2129,
     "status": "ok",
     "timestamp": 1595601574557,
     "user": {
      "displayName": "Θωμάς Γαβριηλίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyDo4VeKpT2F9X9GW7jS6oa8y3-fFX_M4rRpsF=s64",
      "userId": "00428069651110920473"
     },
     "user_tz": -180
    },
    "id": "JnqmcR1qQ5Kq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import confusion_matrix, classification_report,precision_score,f1_score,recall_score,accuracy_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to save the results\n",
    "LBPH_results = dict({'bioid':None,'lfw':None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes=np.empty(5)\n",
    "train_sizes[:]=np.NaN\n",
    "train_scores=np.empty([5, 10])\n",
    "train_scores[:]=np.NaN\n",
    "test_scores=np.empty([5, 10])\n",
    "test_scores[:]=np.NaN\n",
    "fit_times=np.empty([5, 10])\n",
    "fit_times[:]=np.NaN\n",
    "score_times=np.empty([5, 10])\n",
    "score_times[:]=np.NaN\n",
    "lbph_learning_curve=dict({1:train_sizes,2:train_scores,3:test_scores,4:fit_times,5:score_times})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection(image):\n",
    "    #image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haar_classifier = cv2.CascadeClassifier('C:/Users/Thomas/Documents/face_detection/haarcascades/haarcascade_frontalface_default.xml')\n",
    "    face = haar_classifier.detectMultiScale(image,minNeighbors=6,minSize=(75, 75)) #parameters change depending the dataset\n",
    "    if (len(face)==1): #if 1 face is detected\n",
    "        (x,y,w,h) = face[0]\n",
    "        return True, image[y:y+w, x:x+h], face[0]\n",
    "    elif (len(face)==0): #if no faces are detected\n",
    "        \n",
    "        return False, \"zero\"\n",
    "    else: #if more than 1 faces are detected, the biggest is kept (the smaller ones usually belong in the background)\n",
    "        biggest=0\n",
    "        pos=0\n",
    "        for i,(x,y,w,h) in enumerate(face):\n",
    "            if biggest<(h*w):\n",
    "                biggest=h*w\n",
    "                pos=i\n",
    "        return True, image[y:y+w, x:x+h], face[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection(image):\n",
    "    #image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haar_classifier = cv2.CascadeClassifier('C:/Users/Thomas/Documents/face_detection/haarcascades/haarcascade_frontalface_default.xml')\n",
    "    face = haar_classifier.detectMultiScale(image)#,scaleFactor=1.2, minNeighbors=5)\n",
    "    if (len(face)==1): #if 1 face is found\n",
    "        (x,y,w,h) = face[0]\n",
    "        '''\n",
    "        for (x,y,w,h) in face: \n",
    "            cv2.rectangle(image, (x,y), (x+w,y+h), (255,255,255), 3)\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        '''\n",
    "        return True, image[y:y+w, x:x+h], face[0]\n",
    "    elif (len(face)==0): #if no faces are detected\n",
    "        return False, \"zero\"\n",
    "    else: #if more than 1 faces are detected\n",
    "        return False, \"more\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_detection(image):\n",
    "    #image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image=cv2.resize(image, (200, 200), interpolation=cv2.INTER_LINEAR)\n",
    "    haar_classifier = cv2.CascadeClassifier('C:/Users/Thomas/Documents/face_detection/haarcascades/haarcascade_eye.xml')\n",
    "    eyes = haar_classifier.detectMultiScale(image, minNeighbors=4)#, minSize=(30, 30))\n",
    "    if (len(eyes)==2): #if 2 eyes are detected\n",
    "        #(x,y,w,h) = face[0]\n",
    "        return True, eyes\n",
    "    else:      #if the number of eyes found is different from 2\n",
    "        return False, \"more or less\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=('C:/Users/Thomas/Documents/Datasets/ExtYale(640x480)/')\n",
    "min_faces=30\n",
    "scores=[]\n",
    "\n",
    "names=[\"accuracy\",\"macro_avg_precision\",\"macro_avg_recall\",\"macro_avg_f1_score\",\n",
    "       \"micro_avg_precision\",\"micro_avg_recall\",\"micro_avg_f1_score\",\n",
    "       \"weighted_avg_precision\",\"weighted_avg_recall\",\"weighted_avg_f1_score\",\n",
    "       \"fit_time\"]\n",
    "\n",
    "df = pd.DataFrame(index=names)\n",
    "\n",
    "target_names=[] #all the names are saved\n",
    "total_photos_seen=0 \n",
    "n_classes=0\n",
    "folders = os.listdir(datapath)\n",
    "for folder in folders:\n",
    "    label = os.path.basename(folder)\n",
    "    training_images_path = datapath + '/' + folder\n",
    "    num_of_faces = len(os.listdir(training_images_path))\n",
    "    #if num_of_faces>=min_faces:   #people with low number of faces are skipped\n",
    "    target_names.append(label)\n",
    "    n_classes=n_classes+1\n",
    "    faces_per_person=0\n",
    "    for image in os.listdir(training_images_path):\n",
    "        total_photos_seen=total_photos_seen+1\n",
    "        image_path = training_images_path + '/' + image\n",
    "        training_image = cv2.imread(image_path)\n",
    "        #face=cv2.cvtColor(training_image,cv2.COLOR_BGR2GRAY)\n",
    "        #resized=cv2.resize(face,(224,224),interpolation=cv2.INTER_AREA)\n",
    "        result_im=face_detection(training_image) #this function returns TRUE/FALSE, the cut image and the coordinates of where the cut was made\n",
    "        if result_im[0]: #if TRUE (face detected) then the cut face goes into the array\n",
    "            cut_face=result_im[1]\n",
    "            faces.append(cut_face)\n",
    "            labels.append(n_classes)\n",
    "\n",
    "        '''\n",
    "        if faces_per_person==45:\n",
    "                break\n",
    "        '''       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_faces=[]\n",
    "new_labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_faces=0\n",
    "for i in range(len(faces)):\n",
    "    training_image = cv2.imread(faces[i])\n",
    "    result_im=eye_detection(training_image) #finding the place of the eyes\n",
    "    \n",
    "    if result_im[0]:  #process of finding the angle of the two eyes, and alighnment\n",
    "        rotated_faces=rotated_faces+1\n",
    "        (x1,y1,w1,h1)=result_im[1][0]\n",
    "        (x2,y2,w2,h2)=result_im[1][1]\n",
    "        eye1_center_x=x1+(w1/2)\n",
    "        eye1_center_y=y1+(h1/2)\n",
    "        eye2_center_x=x2+(w2/2)\n",
    "        eye2_center_y=y2+(h2/2)\n",
    "        if (eye1_center_x>eye2_center_y):\n",
    "            deltaY = eye1_center_y - eye2_center_y \n",
    "            deltaX = eye1_center_x - eye2_center_x\n",
    "        else:\n",
    "            deltaY = eye2_center_y - eye1_center_y \n",
    "            deltaX = eye2_center_x - eye1_center_x\n",
    "        my_radians = math.atan2(deltaY , deltaX)\n",
    "        my_degrees = math.degrees(my_radians)\n",
    "\n",
    "        #rotation\n",
    "        rows, cols = training_image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((cols/2, rows/2), my_degrees, 1)\n",
    "        img_rotated = cv2.warpAffine(training_image, M, (cols,rows))\n",
    "        '''\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        plt.text(0,0,i)\n",
    "        plt.imshow(img_rotated)\n",
    "        plt.show()\n",
    "        '''\n",
    "\n",
    "        #scaling (all images will become 200x200)\n",
    "        #if before the conversion the image is smaller than 200x200 INTER_LINEAR will be used, else INTER_AREA will be used\n",
    "        image_size=rows*cols        \n",
    "        if (image_size<=40000):\n",
    "            scaled_image=cv2.resize(img_rotated, (200, 200), interpolation=cv2.INTER_LINEAR)\n",
    "        else:\n",
    "            scaled_image=cv2.resize(img_rotated, (200, 200), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        #normalization\n",
    "        norm_img = np.zeros((200, 200))\n",
    "        norm_img = cv2.normalize(scaled_image, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        final_image = cv2.cvtColor(norm_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        new_faces.append(final_image)\n",
    "        new_labels.append(labels[i])\n",
    "        \n",
    "faces=new_faces\n",
    "labels=new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=StratifiedKFold(n_splits=10)\n",
    "\n",
    "for k_f,idx in enumerate(kf.split(X=faces, y=labels)):\n",
    "    train_idx,test_idx=idx[0],idx[1]\n",
    "    X_train=faces[train_idx]\n",
    "    y_train=labels[train_idx]\n",
    "\n",
    "    X_test=faces[test_idx]\n",
    "    y_test=labels[test_idx]\n",
    "\n",
    "    print(\"Fold Number : \",k_f+1)  \n",
    "    #in every loop, different folds are used\n",
    "    #in every loop different models are trained with different training set sizes (in the end with all the data), in order to find the data for the learning curve\n",
    "    #for this to happen, a random part of each fold is selected in every loop\n",
    "    \n",
    "    #14% size training===========================================================================================================\n",
    "    X_train_10, X, y_train_10, y_ = train_test_split(X_train, y_train, train_size=0.22,shuffle=True,stratify=y_train)\n",
    "    X_, X_test_10, y_, y_test_10 = train_test_split(X_test, y_test, test_size=0.22,shuffle=True,stratify=y_test)\n",
    "\n",
    "    lbph_learning_curve[1][0]=len(X_train_10)#setting train_size\n",
    "    \n",
    "    print(\"Fitting the recognizer to the training set=====14% size\")\n",
    "    t0 = time()\n",
    "    model = cv2.face.LBPHFaceRecognizer_create()\n",
    "    model.train(X_train_10, y_train_10)\n",
    "    fit_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %fit_time )\n",
    "    lbph_learning_curve[4][0][k_f]=fit_time#setting fit_time\n",
    "    \n",
    "    print(\"Finding train scores=====14% size\")\n",
    "    train_preds=[]\n",
    "    for image in X_train_10:\n",
    "        prediction = model.predict(image)\n",
    "        train_preds.append(prediction[0])\n",
    "    lbph_learning_curve[2][0][k_f]=accuracy_score(y_train_10, train_preds)#setting train_score\n",
    "    \n",
    "    print(\"Predicting people's names on the test set=====14% size\")\n",
    "    t0 = time()\n",
    "    y_pred=[]\n",
    "    for image in X_test_10:\n",
    "        prediction = model.predict(image)\n",
    "        y_pred.append(prediction[0])\n",
    "    score_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %score_time)\n",
    "    lbph_learning_curve[3][0][k_f]=accuracy_score(y_test_10, y_pred)#setting test_score\n",
    "    lbph_learning_curve[5][0][k_f]=score_time#setting score_times\n",
    "    \n",
    "    \n",
    "    \n",
    "    #32.5% size training===========================================================================================================\n",
    "    X_train_325, X, y_train_325, y_ = train_test_split(X_train, y_train, train_size=0.325,shuffle=True,stratify=y_train)\n",
    "    X_, X_test_325, y_, y_test_325 = train_test_split(X_test, y_test, test_size=0.325,shuffle=True,stratify=y_test)\n",
    "    \n",
    "    lbph_learning_curve[1][1]=len(X_train_325)#setting train_size\n",
    "    \n",
    "    print(\"Fitting the recognizer to the training set=====32.5% size\")\n",
    "    t0 = time()\n",
    "    model = cv2.face.LBPHFaceRecognizer_create()\n",
    "    model.train(X_train_325, y_train_325)\n",
    "    fit_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %fit_time )\n",
    "    lbph_learning_curve[4][1][k_f]=fit_time#setting fit_time\n",
    "    \n",
    "    print(\"Finding train scores=====32.5% size\")\n",
    "    train_preds=[]\n",
    "    for image in X_train_325:\n",
    "        prediction = model.predict(image)\n",
    "        train_preds.append(prediction[0])\n",
    "    lbph_learning_curve[2][1][k_f]=accuracy_score(y_train_325, train_preds)#setting train_score\n",
    "    \n",
    "    print(\"Predicting people's names on the test set=====32.5% size\")\n",
    "    t0 = time()\n",
    "    y_pred=[]\n",
    "    for image in X_test_325:\n",
    "        prediction = model.predict(image)\n",
    "        y_pred.append(prediction[0])\n",
    "    score_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %score_time)\n",
    "    lbph_learning_curve[3][1][k_f]=accuracy_score(y_test_325, y_pred)#setting test_score\n",
    "    lbph_learning_curve[5][1][k_f]=score_time#setting score_times\n",
    "    \n",
    "    #55% size training===========================================================================================================\n",
    "    X_train_55, X, y_train_55, y = train_test_split(X_train, y_train, train_size=0.55,shuffle=True,stratify=y_train)\n",
    "    X_, X_test_55, y_, y_test_55 = train_test_split(X_test, y_test, test_size=0.55,shuffle=True,stratify=y_test)\n",
    "\n",
    "    lbph_learning_curve[1][2]=len(X_train_55)#setting train_size\n",
    "    \n",
    "    print(\"Fitting the recognizer to the training set=====55% size\")\n",
    "    t0 = time()\n",
    "    model = cv2.face.LBPHFaceRecognizer_create()\n",
    "    model.train(X_train_55, y_train_55)\n",
    "    fit_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %fit_time )\n",
    "    lbph_learning_curve[4][2][k_f]=fit_time#setting fit_time\n",
    "    \n",
    "    print(\"Finding train scores=====55% size\")\n",
    "    train_preds=[]\n",
    "    for image in X_train_55:\n",
    "        prediction = model.predict(image)\n",
    "        train_preds.append(prediction[0])\n",
    "    lbph_learning_curve[2][2][k_f]=accuracy_score(y_train_55, train_preds)#setting train_score\n",
    "    \n",
    "    print(\"Predicting people's names on the test set=====55% size\")\n",
    "    t0 = time()\n",
    "    y_pred=[]\n",
    "    for image in X_test_55:\n",
    "        prediction = model.predict(image)\n",
    "        y_pred.append(prediction[0])\n",
    "    score_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %score_time)\n",
    "    lbph_learning_curve[3][2][k_f]=accuracy_score(y_test_55, y_pred)#setting test_score\n",
    "    lbph_learning_curve[5][2][k_f]=score_time#setting score_times\n",
    "    \n",
    "    #77.5% size training===========================================================================================================\n",
    "    X_train_775, X, y_train_775, y_ = train_test_split(X_train, y_train, train_size=0.77,shuffle=True,stratify=y_train)\n",
    "    X_, X_test_775, y_, y_test_775 = train_test_split(X_test, y_test, test_size=0.77,shuffle=True,stratify=y_test)\n",
    "    \n",
    "    lbph_learning_curve[1][3]=len(X_train_775)#setting train_size\n",
    "    \n",
    "    print(\"Fitting the recognizer to the training set=====77.5% size\")\n",
    "    t0 = time()\n",
    "    model = cv2.face.LBPHFaceRecognizer_create()\n",
    "    model.train(X_train_775, y_train_775)\n",
    "    fit_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %fit_time )\n",
    "    lbph_learning_curve[4][3][k_f]=fit_time#setting fit_time\n",
    "    \n",
    "    print(\"Finding train scores=====77.5% size\")\n",
    "    train_preds=[]\n",
    "    for image in X_train_775:\n",
    "        prediction = model.predict(image)\n",
    "        train_preds.append(prediction[0])\n",
    "    lbph_learning_curve[2][3][k_f]=accuracy_score(y_train_775, train_preds)#setting train_score\n",
    "    \n",
    "    print(\"Predicting people's names on the test set=====77.5% size\")\n",
    "    t0 = time()\n",
    "    y_pred=[]\n",
    "    for image in X_test_775:\n",
    "        prediction = model.predict(image)\n",
    "        y_pred.append(prediction[0])\n",
    "    score_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %score_time)\n",
    "    lbph_learning_curve[3][3][k_f]=accuracy_score(y_test_775, y_pred)#setting test_score\n",
    "    lbph_learning_curve[5][3][k_f]=score_time#setting score_times\n",
    "    \n",
    "    #100% size=========================================================================================================\n",
    "    \n",
    "    lbph_learning_curve[1][4]=len(X_train)#setting train_size\n",
    "    \n",
    "    print(\"Fitting the recognizer to the training set========full size\")\n",
    "    t0 = time()\n",
    "    model = cv2.face.LBPHFaceRecognizer_create()\n",
    "    model.train(X_train, y_train)\n",
    "    fit_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %fit_time )\n",
    "    lbph_learning_curve[4][4][k_f]=fit_time#setting fit_time\n",
    "    \n",
    "    print(\"Finding train scores=====full size\")\n",
    "    train_preds=[]\n",
    "    for image in X_train:\n",
    "        prediction = model.predict(image)\n",
    "        train_preds.append(prediction[0])\n",
    "    lbph_learning_curve[2][4][k_f]=accuracy_score(y_train, train_preds)#setting train_score\n",
    "    \n",
    "\n",
    "    print(\"Predicting people's names on the test set=====full size\")\n",
    "    t0 = time()\n",
    "    y_pred=[]\n",
    "    for image in X_test:\n",
    "        prediction = model.predict(image)\n",
    "        y_pred.append(prediction[0])\n",
    "    score_time=time() - t0\n",
    "    print(\"done in %0.3fs\" %score_time)\n",
    "    lbph_learning_curve[3][4][k_f]=accuracy_score(y_test, y_pred)#setting test_score\n",
    "    lbph_learning_curve[5][4][k_f]=score_time#setting score_times\n",
    "\n",
    "    report=classification_report(y_test, y_pred,output_dict=True)\n",
    "    accuracy=report['accuracy']\n",
    "    macro_avg_precision=report['macro avg']['precision']\n",
    "    macro_avg_recall=report['macro avg']['recall']\n",
    "    macro_avg_f1_score=report['macro avg']['f1-score']\n",
    "    micro_avg_precision=precision_score(y_test, y_pred, average='micro')\n",
    "    micro_avg_recall=recall_score(y_test, y_pred, average='micro')\n",
    "    micro_avg_f1_score=f1_score(y_test, y_pred, average='micro')\n",
    "    weighted_avg_precision=report['weighted avg']['precision']\n",
    "    weighted_avg_recall=report['weighted avg']['recall']\n",
    "    weighted_avg_f1_score=report['weighted avg']['f1-score']\n",
    "    \n",
    "    #scores.append((accuracy,macro_avg_precision,macro_avg_recall,macro_avg_f1_score,weighted_avg_precision,weighted_avg_recall,weighted_avg_f1_score))\n",
    "    df[k_f+1]=[accuracy,macro_avg_precision,macro_avg_recall,macro_avg_f1_score,\n",
    "               micro_avg_precision,micro_avg_recall,micro_avg_f1_score,\n",
    "               weighted_avg_precision,weighted_avg_recall,weighted_avg_f1_score,\n",
    "               fit_time]\n",
    "    \n",
    "print(\"Loop Done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('lbph_bioid.p', 'wb') as fp:\n",
    "#        pickle.dump(df_bioid, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "df.to_pickle('lbph')\n",
    "#with open('lbph_learning_curve_bioid.p', 'wb') as fp:\n",
    "#        pickle.dump(lbph_learning_curve_bioid, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lbph_learning_curve.p', 'wb') as fp:\n",
    "        pickle.dump(lbph_learning_curve, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LBPH.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
