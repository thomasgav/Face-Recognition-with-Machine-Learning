{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2143,
     "status": "ok",
     "timestamp": 1606561538708,
     "user": {
      "displayName": "Θωμάς Γαβριηλίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyDo4VeKpT2F9X9GW7jS6oa8y3-fFX_M4rRpsF=s64",
      "userId": "00428069651110920473"
     },
     "user_tz": -120
    },
    "id": "7ysyaGZEhSGI"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2305,
     "status": "ok",
     "timestamp": 1606561727438,
     "user": {
      "displayName": "Θωμάς Γαβριηλίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyDo4VeKpT2F9X9GW7jS6oa8y3-fFX_M4rRpsF=s64",
      "userId": "00428069651110920473"
     },
     "user_tz": -120
    },
    "id": "xfItQ1RbhSGJ"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YcgWFGhhSGJ"
   },
   "outputs": [],
   "source": [
    "train_path = 'C:/Users/Thomas/Documents/Datasets/lfw(250x250)/'  \n",
    "training_names = os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCT9qw2EhSGc"
   },
   "source": [
    "Image import for parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 672,
     "status": "ok",
     "timestamp": 1606561608112,
     "user": {
      "displayName": "Θωμάς Γαβριηλίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyDo4VeKpT2F9X9GW7jS6oa8y3-fFX_M4rRpsF=s64",
      "userId": "00428069651110920473"
     },
     "user_tz": -120
    },
    "id": "jThOUh1OhSGc",
    "outputId": "32c840f7-ce41-482e-f940-4433d9f794ee"
   },
   "outputs": [],
   "source": [
    "#in these arrays the different folds are saved\n",
    "face_data=[[],[],[],[],[]]\n",
    "label_data=[[],[],[],[],[]]\n",
    "kfol=5\n",
    "\n",
    "print(\"Reading Images...\")\n",
    "t0 = time()\n",
    "min_faces=30\n",
    "target_names=[]\n",
    "total_photos_seen=0\n",
    "n_classes=0\n",
    "folders = os.listdir(train_path)\n",
    "for folder in folders:\n",
    "    label = os.path.basename(folder)\n",
    "    training_images_path = train_path + '/' + folder\n",
    "    num_of_faces = len(os.listdir(training_images_path))\n",
    "    if num_of_faces>=min_faces:   #people with low number of faces are skipped\n",
    "        n_classes=n_classes+1\n",
    "        target_names.append(label)\n",
    "        faces=[]\n",
    "        labels=[]\n",
    "        for image in os.listdir(training_images_path):\n",
    "            total_photos_seen=total_photos_seen+1\n",
    "            image_path = training_images_path + '/' + image\n",
    "            faces.append(image_path)\n",
    "            labels.append(n_classes)\n",
    "        seed(1)\n",
    "        face_folds,label_folds = cross_validation_split(faces,labels, kfol) #here the different folds are created\n",
    "        for i in range(0,kfol):\n",
    "            face_data[i].extend(face_folds[i])\n",
    "            label_data[i].extend(label_folds[i])\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lB5gzQsAhSGc"
   },
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLIt2mothSGm",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_svc(trial,x,y):\n",
    "    svc__C=trial.suggest_uniform(\"C\",0.001,100)\n",
    "    svc__gamma=trial.suggest_uniform(\"gamma\",0.0001,100)\n",
    "    svc__kernel=trial.suggest_categorical(\"kernel\",[\"rbf\",\"linear\", \"poly\",\"sigmoid\"])\n",
    "    svc__degree=trial.suggest_int(\"degree\",1,6)\n",
    "    model=SVC(class_weight='balanced',\n",
    "           C=svc__C,\n",
    "           gamma=svc__gamma,\n",
    "           kernel=svc__kernel,\n",
    "           degree=svc__degree)\n",
    "    #kf=StratifiedKFold(n_splits=5)\n",
    "    kfol=5\n",
    "    accuracies=[]\n",
    "    for k_f in range (1,kfol+1):\n",
    "        print(\"Fold Number : \",k_f)\n",
    "        #Training\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMzcYe6khSGm"
   },
   "outputs": [],
   "source": [
    "optimization_function_svc=partial(optimize_svc,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp19aElohSGm",
    "outputId": "fd1ca938-abf6-405d-93b3-e0a62d0d01f9"
   },
   "outputs": [],
   "source": [
    "study_svc=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQM8V5z5hSGm",
    "outputId": "64134fba-894d-41b0-ca05-2ac348d122c8"
   },
   "outputs": [],
   "source": [
    "study_svc.optimize(optimization_function_svc,n_trials=150,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zH47eXqhSGo",
    "outputId": "5129acf2-6eb1-464f-952f-935ad9263a61"
   },
   "outputs": [],
   "source": [
    "print(study_svc.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiRkxIbRhSGo"
   },
   "source": [
    "LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFLbwMNOhSGo",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_linsvc(trial,x,y):\n",
    "    C=trial.suggest_uniform(\"C\",0.001,100)\n",
    "    loss=trial.suggest_categorical(\"loss\",['hinge', 'squared_hinge'])\n",
    "    class_weight=trial.suggest_categorical(\"class_weight\",[None, 'balanced'])\n",
    "    fit_intercept=trial.suggest_categorical(\"fit_intercept\",[True,False])\n",
    "    model = LinearSVC(C=C,loss=loss,fit_intercept=fit_intercept,class_weight=class_weight)\n",
    "    kfol=5\n",
    "    accuracies=[]\n",
    "    for k_f in range (1,kfol+1):\n",
    "        print(\"Fold Number : \",k_f)\n",
    "        #Training\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQAeRQymhSGo"
   },
   "outputs": [],
   "source": [
    "optimization_function_linsvc=partial(optimize_linsvc,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACVgCx_uhSGp",
    "outputId": "22e66ccd-6018-4293-a438-bf431a54a75c"
   },
   "outputs": [],
   "source": [
    "study_linsvc=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C4779-vhSGp",
    "outputId": "3f5edb16-b275-4dcd-fa21-9d8dc979c186"
   },
   "outputs": [],
   "source": [
    "study_linsvc.optimize(optimization_function_linsvc,n_trials=150,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlkMqSe3hSGr",
    "outputId": "bdd965b3-1941-442b-8883-1c73a57cbe9c"
   },
   "outputs": [],
   "source": [
    "print(study_linsvc.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaaeZmTvhSGs"
   },
   "source": [
    "Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ozr6fdUPhSGt",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_forrest(trial,x,y):\n",
    "    #impurity_decrease=trial.suggest_uniform(\"impurity_decrease\",0.0,0.3)\n",
    "    #min_impurity_split=trial.suggest_uniform(\"min_impurity_split\",,)\n",
    "    list_max_depth=[None,2,3,4,5,6,7,8,9,10] \n",
    "    max_depth=trial.suggest_categorical(\"max_depth\",list_max_depth)\n",
    "    min_samples_leaf=trial.suggest_int(\"min_samples_leaf\",1,4)\n",
    "    n_estimators=trial.suggest_int(\"n_estimators\",10,600)\n",
    "    class_weight=trial.suggest_categorical(\"class_weight\",[None, 'balanced'])\n",
    "    bootstrap=trial.suggest_categorical(\"bootstrap\",[False, True])\n",
    "    model = RandomForestClassifier(random_state=0,\n",
    "                                    max_depth=max_depth,\n",
    "                                    bootstrap=bootstrap,\n",
    "                                    class_weight=class_weight,\n",
    "                                    min_samples_leaf=min_samples_leaf,\n",
    "                                    n_estimators=n_estimators)\n",
    "    kfol=5\n",
    "    accuracies=[]\n",
    "    for k_f in range (1,kfol+1):\n",
    "        print(\"Fold Number : \",k_f)\n",
    "        #Training\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "585clPIShSGt"
   },
   "outputs": [],
   "source": [
    "optimization_function_forrest=partial(optimize_forrest,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPAJa49nhSGt",
    "outputId": "7df7f4c8-3d21-4aa9-b629-1d38f1acc6c3"
   },
   "outputs": [],
   "source": [
    "study_forrest=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJo0FssthSGt",
    "outputId": "1e7ef8ae-156e-4ea7-84a8-1d5e053bcf27"
   },
   "outputs": [],
   "source": [
    "study_forrest.optimize(optimization_function_forrest,n_trials=150,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tN7e4WB8hSGu",
    "outputId": "e5dcaf03-0d7b-4665-fc00-b9bb166cfe1b"
   },
   "outputs": [],
   "source": [
    "print(study_forrest.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP8x78uxhSGu"
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBopYSejhSGu",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_logreg(trial,x,y):\n",
    "    class_weight=trial.suggest_categorical(\"class_weight\",[None,\"balanced\"])\n",
    "    C=trial.suggest_uniform(\"C\",0.001,100)\n",
    "    solver=trial.suggest_categorical(\"solver\",['newton-cg', 'lbfgs', 'sag', 'saga'])\n",
    "    model = LogisticRegression(C=C,class_weight=class_weight,solver=solver)\n",
    "\n",
    "    kfol=5\n",
    "    accuracies=[]\n",
    "    for k_f in range (1,kfol+1):\n",
    "        print(\"Fold Number : \",k_f)\n",
    "        #Training\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqLWcRZyhSGu"
   },
   "outputs": [],
   "source": [
    "optimization_function_logreg=partial(optimize_logreg,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj79AzgzhSGu",
    "outputId": "44129c4f-9a58-4d3d-8ab6-828344904f5f"
   },
   "outputs": [],
   "source": [
    "study_logreg=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7ODSK4KhSGu",
    "outputId": "730fda4d-c554-459f-f5d5-185028e6d498"
   },
   "outputs": [],
   "source": [
    "study_logreg.optimize(optimization_function_logreg,n_trials=150,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OuQQFfkhSGv",
    "outputId": "270f1aeb-22b4-4572-c951-1583de949c15"
   },
   "outputs": [],
   "source": [
    "print(study_logreg.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_FacxgnhSGw"
   },
   "source": [
    "Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI2WPq3AhSGw",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_trees(trial,x,y):\n",
    "    list_max_depth=[None,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    list_max_leaf_nodes=[None,2,3,4,5,6,7]\n",
    "    list_max_features=[None,1,2,3,4,5,6,7,8,9,10]\n",
    "    \n",
    "    criterion=trial.suggest_categorical(\"criterion\",[\"gini\",\"entropy\"])\n",
    "    min_impurity_decrease=trial.suggest_uniform(\"min_impurity_decrease\",0.0,0.3)\n",
    "    max_depth=trial.suggest_categorical(\"max_depth\",list_max_depth)\n",
    "    min_samples_leaf=trial.suggest_int(\"min_samples_leaf\",1,10)\n",
    "    max_leaf_nodes=trial.suggest_categorical(\"max_leaf_nodes\",list_max_leaf_nodes)\n",
    "    max_features=trial.suggest_categorical(\"max_features\",list_max_features)\n",
    "    model = DecisionTreeClassifier(random_state=0,\n",
    "                                 min_impurity_decrease=min_impurity_decrease,\n",
    "                                 max_depth=max_depth,\n",
    "                                 min_samples_leaf=min_samples_leaf,\n",
    "                                 max_leaf_nodes=max_leaf_nodes,\n",
    "                                 max_features=max_features,\n",
    "                                 criterion=criterion)\n",
    "    kfol=5\n",
    "    accuracies=[]\n",
    "    for k_f in range (1,kfol+1):\n",
    "        print(\"Fold Number : \",k_f)\n",
    "        #Training\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa7F1JYjhSGw"
   },
   "outputs": [],
   "source": [
    "optimization_function_trees=partial(optimize_trees,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a6Z_RK5hSGw",
    "outputId": "4a07a293-753e-46e7-84bc-4b50a22726b0"
   },
   "outputs": [],
   "source": [
    "study_trees=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4h8-bW2hSGw",
    "outputId": "1bd74849-a2bb-44bc-fcbe-368180b19a99"
   },
   "outputs": [],
   "source": [
    "study_trees.optimize(optimization_function_trees,n_trials=150,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMCUw7mDhSGw",
    "outputId": "6dd2adae-6da8-4696-e5b3-793f00b283ec"
   },
   "outputs": [],
   "source": [
    "print(study_trees.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYNLsYHghSGx"
   },
   "source": [
    "kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYiYExexhSGx",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_knn(trial,x,y):\n",
    "    n_neighbors=trial.suggest_int(\"n_neighbors\",1,15)\n",
    "    p=trial.suggest_int(\"p\",1,5)\n",
    "    leaf_size=trial.suggest_int(\"leaf_size\",10,50)\n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbors,p=p,leaf_size=leaf_size)\n",
    "    accuracies=[]\n",
    "    kfol=5\n",
    "    for k_f in range (1,kfol+1):\n",
    "        print(\"Fold Number : \",k_f)\n",
    "        #Training\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp4we1EJhSGx"
   },
   "outputs": [],
   "source": [
    "optimization_function_knn=partial(optimize_knn,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYsmWTmzhSGx",
    "outputId": "f70e3804-c6e7-49f6-fe1a-c6dd30cc9d4f"
   },
   "outputs": [],
   "source": [
    "study_knn=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxOJsWP5hSGx",
    "outputId": "af90114c-c248-4b8c-c0fd-4edf0e633c5b"
   },
   "outputs": [],
   "source": [
    "study_knn.optimize(optimization_function_knn,n_trials=150,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kgU-Ae0hSGy",
    "outputId": "20a7c6dd-ef9a-49ac-d37a-6118c1de3687"
   },
   "outputs": [],
   "source": [
    "print(study_knn.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGLhaaeGhSGy"
   },
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1606561638643,
     "user": {
      "displayName": "Θωμάς Γαβριηλίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyDo4VeKpT2F9X9GW7jS6oa8y3-fFX_M4rRpsF=s64",
      "userId": "00428069651110920473"
     },
     "user_tz": -120
    },
    "id": "8C5psKychSGy",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_mlp(trial,x,y):\n",
    "    n_layers = trial.suggest_int('n_layers', 1,10)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f'n_units_{i}', 1, 500))\n",
    "    \n",
    "    alpha=trial.suggest_uniform(\"alpha\",0.0001,0.05)\n",
    "    learning_rate=trial.suggest_categorical(\"learning_rate\",[\"constant\",\"adaptive\"])\n",
    "    activation=trial.suggest_categorical(\"activation\",[\"tanh\",\"relu\"])\n",
    "    solver=trial.suggest_categorical(\"solver\",[\"sgd\",\"adam\"])\n",
    "    momentum = trial.suggest_uniform('momentum', 0.0, 1.0)\n",
    "    \n",
    "    model = MLPClassifier(solver=solver,activation=activation,hidden_layer_sizes=tuple(layers),learning_rate=learning_rate,alpha=alpha,momentum=momentum, verbose=0, early_stopping=True)\n",
    "    #kf=StratifiedKFold(n_splits=5)\n",
    "    kfol=5\n",
    "    accuracies=[]\n",
    "    for k_f in range (1,kfol+1):\n",
    "\n",
    "        print(\"Fold Number : \",k_f)\n",
    "\n",
    "        #Training\n",
    "\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1606561640631,
     "user": {
      "displayName": "Θωμάς Γαβριηλίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyDo4VeKpT2F9X9GW7jS6oa8y3-fFX_M4rRpsF=s64",
      "userId": "00428069651110920473"
     },
     "user_tz": -120
    },
    "id": "DX-OBPlrhSGy"
   },
   "outputs": [],
   "source": [
    "optimization_function_mlp=partial(optimize_mlp,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1606561738800,
     "user": {
      "displayName": "Θωμάς Γαβριηλίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyDo4VeKpT2F9X9GW7jS6oa8y3-fFX_M4rRpsF=s64",
      "userId": "00428069651110920473"
     },
     "user_tz": -120
    },
    "id": "qtl-FujkhSGy",
    "outputId": "e7ee5728-0e11-4685-8a5a-86f4baa2b9a4"
   },
   "outputs": [],
   "source": [
    "study_mlp=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4nb4vSdhSGy"
   },
   "outputs": [],
   "source": [
    "study_mlp.optimize(optimization_function_mlp,n_trials=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q98X0KihhSGy"
   },
   "outputs": [],
   "source": [
    "print(study_mlp.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqYPZaW7hSGy"
   },
   "source": [
    "AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWntTVi9hSGy",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_ada(trial,x,y):\n",
    "    n_estimators=trial.suggest_int(\"n_estimators\",50,300)\n",
    "    learning_rate=trial.suggest_uniform(\"learning_rate\",0.001,1.0)\n",
    "    model = AdaBoostClassifier(random_state=0,\n",
    "                                 n_estimators=n_estimators,\n",
    "                                 learning_rate=learning_rate)\n",
    "    kfol=5\n",
    "    accuracies=[]\n",
    "    for k_f in range (1,kfol+1):\n",
    "        print(\"Fold Number : \",k_f)\n",
    "        #Training\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DATyK67BhSGy"
   },
   "outputs": [],
   "source": [
    "optimization_function_ada=partial(optimize_ada,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ywgzFARhSGy"
   },
   "outputs": [],
   "source": [
    "study_ada=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OL6nMHDdhSGy"
   },
   "outputs": [],
   "source": [
    "study_ada.optimize(optimization_function_ada,n_trials=150,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkBsuHI-hSGy",
    "outputId": "bafb8a27-6de4-408e-fbf4-20a6b6ee678c"
   },
   "outputs": [],
   "source": [
    "print(study_ada.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CPDoRR2hSG0"
   },
   "source": [
    "GaussianNB (no need for tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlr-Nm2jhSG0"
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_gauss(trial,x,y):\n",
    "    #kernel_l=[\"DotProduct\",\"Matern\",\"RBF\",\"RationalQuadratic\",\"WhiteKernel\"]\n",
    "    #kernel = trial.suggest_categorical(\"kernel\", kernel_l)\n",
    "    \n",
    "#    if kernel==\"DotProduct\":\n",
    "#        sigma_0=trial.suggest_uniform(\"sigma_0\",0.2,3.0)\n",
    " #       gpc = GaussianProcessClassifier(kernel=1.0*DotProduct(sigma_0=sigma_0),\n",
    " #                                   random_state=0)\n",
    "#    elif kernel==\"Matern\":\n",
    " #       length_scale=trial.suggest_uniform(\"length_scale\",0.2,3.0)\n",
    "#        gpc = GaussianProcessClassifier(kernel=1.0*Matern(length_scale=length_scale),\n",
    "#                                    random_state=0)\n",
    "#    elif kernel==\"RBF\":\n",
    "    length_scale=trial.suggest_uniform(\"length_scale\",0.2,3.0)\n",
    "    model = GaussianProcessClassifier(kernel=1.0*RBF(length_scale=length_scale),\n",
    "                                random_state=0)\n",
    "#    elif kernel==\"RationalQuadratic\":\n",
    "#        length_scale=trial.suggest_uniform(\"length_scale\",0.2,3.0)\n",
    "#        gpc = GaussianProcessClassifier(kernel=1.0*RationalQuadratic(length_scale=length_scale),\n",
    "#                                    random_state=0)\n",
    "#    else:\n",
    "#        noise_level=trial.suggest_uniform(\"noise_level\",0.5,1.5)\n",
    "#        gpc = GaussianProcessClassifier(kernel=1.0*WhiteKernel(noise_level=noise_level),\n",
    "#                                    random_state=0)\n",
    "    \n",
    "    #warm_start = trial.suggest_categorical(\"warm_start\", [True,False])\n",
    "    #n_restarts_optimizer = trial.suggest_categorical(\"n_restarts_optimizer\", [0,1,2,3,4,5,6,7,8,9,10])\n",
    "    #copy_X_train = trial.suggest_categorical(\"copy_X_train\", [True,False])\n",
    "    \n",
    "    #gpc = GaussianProcessClassifier(warm_start=warm_start,\n",
    "     #                               n_restarts_optimizer=n_restarts_optimizer,\n",
    "      #                              copy_X_train=copy_X_train,\n",
    "       #                             random_state=0)\n",
    "    kfol=5\n",
    "    accuracies=[]\n",
    "    for k_f in range (1,kfol+1):\n",
    "        print(\"Fold Number : \",k_f)\n",
    "        #Training\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_function_gauss=partial(optimize_gauss,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_gauss=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_gauss.optimize(optimization_function_gauss,n_trials=150,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study_gauss.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_ridge(trial,x,y):\n",
    "    alpha_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    alpha=trial.suggest_categorical(\"alpha\",alpha_list)\n",
    "    solver=trial.suggest_categorical(\"solver\",['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'])  \n",
    "    class_weight=trial.suggest_categorical(\"class_weight\",[None, 'balanced'])\n",
    "\n",
    "    model = RidgeClassifier(alpha=alpha,solver=solver,class_weight=class_weight)\n",
    "    \n",
    "    kfol=5\n",
    "    accuracies=[]\n",
    "    for k_f in range (1,kfol+1):\n",
    "        print(\"Fold Number : \",k_f)\n",
    "        #Training\n",
    "        train_faces=[]\n",
    "        test_faces=[]\n",
    "        train_labels=[]\n",
    "        test_labels=[]\n",
    "\n",
    "        test_faces=face_data[k_f-1]\n",
    "        test_labels=label_data[k_f-1]\n",
    "\n",
    "        for z in range(0,kfol):\n",
    "            if (z!=(k_f-1)):\n",
    "                #print(\"Fold Chosen: \",z+1)\n",
    "                train_faces.extend(face_data[z])\n",
    "                train_labels.extend(label_data[z])\n",
    "\n",
    "        image_paths=train_faces\n",
    "        image_classes=train_labels\n",
    "        test_image_paths=test_faces\n",
    "        test_image_classes=test_labels\n",
    "        des_list = []\n",
    "\n",
    "        orb=cv2.ORB_create()\n",
    "        #brisk = cv2.BRISK_create(30)\n",
    "        #surf=cv2.xfeatures2d.SURF_create()\n",
    "        #sift = cv2.xfeatures2d.SIFT_create(nOctaveLayers=3, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "\n",
    "        list_to_delete=[] \n",
    "\n",
    "        print(\"Finding descriptors for \",len(image_paths),\" training images\")\n",
    "        t0 = time()\n",
    "        for i,image_path in enumerate(image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(image_paths) if i not in list_to_delete]\n",
    "        image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(image_classes) if i not in list_to_delete]\n",
    "        image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(image_paths),\" training images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        descriptors_float = descriptors.astype(float) \n",
    "\n",
    "        print(\"Creating clusters and histogram...\")\n",
    "        t0 = time()\n",
    "        k = 200\n",
    "        voc, variance = kmeans(descriptors_float, k, 1)\n",
    "\n",
    "        im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "        for i in range(len(image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                im_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        stdSlr = StandardScaler().fit(im_features)\n",
    "        im_features = stdSlr.transform(im_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        print(\"Training the model...\")\n",
    "        t0 = time()\n",
    "        model.fit(im_features, np.array(image_classes))        \n",
    "        print(\"Training done in %0.3fs\" % (time() - t0))\n",
    "        \n",
    "        #Testing\n",
    "        \n",
    "        print(\"Finding descriptors for \",len(test_image_paths),\" testing images\")\n",
    "        t0 = time()\n",
    "\n",
    "        des_list = []\n",
    "\n",
    "        list_to_delete=[]\n",
    "\n",
    "        for i,image_path in enumerate(test_image_paths):\n",
    "            im = cv2.imread(image_path)\n",
    "\n",
    "            kpts, des = orb.detectAndCompute(im, None)\n",
    "            #kpts, des = brisk.detectAndCompute(im, None)\n",
    "            #kpts, des = surf.detectAndCompute(im, None)\n",
    "            #kpts, des = sift.detectAndCompute(im, None)\n",
    "\n",
    "            if des is not None:\n",
    "                des_list.append((image_path, des))\n",
    "            else:\n",
    "                list_to_delete.append(i)\n",
    "\n",
    "        new_image_paths = [j for i, j in enumerate(test_image_paths) if i not in list_to_delete]\n",
    "        test_image_paths=new_image_paths\n",
    "\n",
    "        new_image_classes = [j for i, j in enumerate(test_image_classes) if i not in list_to_delete]\n",
    "        test_image_classes=new_image_classes\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "        print(\"Found descriptors for \",len(test_image_paths),\" testing images\")\n",
    "\n",
    "        print(\"Stacking...\")\n",
    "        t0 = time()\n",
    "        descriptors = des_list[0][1]\n",
    "        for image_path, descriptor in des_list[1:]:\n",
    "            descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        print(\"Calculating histogram, Scaling...\")\n",
    "        test_features = np.zeros((len(test_image_paths), k), \"float32\")\n",
    "        for i in range(len(test_image_paths)):\n",
    "            words, distance = vq(des_list[i][1],voc)\n",
    "            for w in words:\n",
    "                test_features[i][w] += 1\n",
    "\n",
    "        nbr_occurences = np.sum( (test_features > 0) * 1, axis = 0)\n",
    "        idf = np.array(np.log((1.0*len(test_image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "        test_features = stdSlr.transform(test_features)\n",
    "        print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "        true_class =  [training_names[i-1] for i in test_image_classes]\n",
    "        predictions =  [training_names[i-1] for i in model.predict(test_features)]\n",
    "        \n",
    "        #preds=model.predict(x_test)\n",
    "        fold_acc=accuracy_score(true_class,predictions)\n",
    "        accuracies.append(fold_acc)\n",
    "        \n",
    "    return - 1.0 * np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_function_ridge=partial(optimize_ridge,x=face_data,y=label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_ridge=optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_ridge.optimize(optimization_function_ridge,n_trials=150,n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study_ridge.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    print(i,\": \",study_ridge.trials[i].value,\", params:\", study_ridge.trials[i].params,\", duration:\",study_ridge.trials[i].duration)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SIFT_SURF_ORB_BRISK_with_HT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
